# Frontend (run the ui_main.py which is linked with the graph.py )
```Bash
streamlit run ui_main.py
```
# Backend (run the backend.py)
```Bash
uvicorn backend:app --reload
```

## âœ… High-Level Workflow (with FastAPI backend)
```Bash
User
  â†“
FastAPI /research-chat endpoint
  â†“
LLM (Intent Detection)
  â†“
Decision Layer (LangGraph / Agent Router)
  â†“
Tools (mp_api, arxiv_api, python, vector search)
  â†“
Raw Data
  â†“
Second LLM Pass (Meta-level decision)
  â†“
Should return to user?
   â†³ Yes â†’ Final Answer
   â†³ No â†’ Loop back â†’ more tools or more reasoning
```

## ðŸ§± Recommended Final Setup
- **User** side (Prod UI):
âœ” Agent ChatUI

Clean, polished, modern conversation UI.

- **Developer** side (Internal UI):
âœ” Streamlit Admin Dashboard

Includes:

LangGraph visualization

Node-by-node breakdown

Tool output previews

Error logs

Retry buttons

Agent version controls

Test prompts

Data inspection

This is what you need for:

debugging

training new agents

adding tools

observability

Aim:




## We needed

---
âœ” Architecture diagram
```Bash
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚          User (Web)           â”‚
                              â”‚    Clean Agent ChatUI (Prod)  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚ HTTPS
                                              â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚         FastAPI API        â”‚
                               â”‚   /research-chat endpoint  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                        LLM Engine                         â”‚
                 â”‚                   (OpenAI / Anthropic)                    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚        Intent Detection          â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚     LangGraph Decision Layer     â”‚
                               â”‚  Router â†’ Nodes â†’ Subgraphs     â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                                   â”‚                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tools Layer        â”‚             â”‚   Python Node       â”‚             â”‚  Vector Store / RAG   â”‚
â”‚ â€¢ mp_api             â”‚             â”‚ â€¢ Data cleaning      â”‚             â”‚ â€¢ FAISS / LanceDB     â”‚
â”‚ â€¢ arxiv_api          â”‚             â”‚ â€¢ Computation        â”‚             â”‚ â€¢ Similarity search   â”‚
â”‚ â€¢ Web search         â”‚             â”‚ â€¢ Graph building     â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Local DB           â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚     Second LLM Pass (Refiner)     â”‚
                               â”‚  â€œShould I answer or continue?â€   â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    Yes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â†’ No â†’ back into LangGraph
                                               â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚      Final Response      â”‚
                                 â”‚  (FastAPI â†’ ChatUI)      â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Parallel Developer-Side Interface:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                                   Streamlit Admin Dashboard
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ LangGraph visualization
â€¢ Node-by-node output
â€¢ Live logs and tool traces
â€¢ Retry last step
â€¢ Edit agent config
â€¢ Test prompts
â€¢ Inspect API responses


```
---
âœ” Streamlit code for LangGraph visualization

---
âœ” Agent ChatUI integration with your FastAPI

---
âœ” Dual-UI folder/project structure

---
âœ” Example LangGraph debugging interface

---




## ðŸŸª Why Agent ChatUI Is Not Ideal right now

Agent ChatUI is:

âœ” Beautiful
âœ” Professional
âœ” Great for a polished ChatGPT-like UX

But it is mainly designed for:

agent-tool execution view

conversation

tool call visualization

It is not designed for:

data tables

plots

materials dashboards

research logs

workflow visualizations

You can use it, but you would lose flexibility.






## ðŸ§  Why a Second LLM Pass Is Important

- This second reasoning step enables:

âœ” Verification

- LLM checks if the retrieved data is sufficient, consistent, and relevant.

âœ” Multi-step research reasoning

Example:

- - User: â€œFind best perovskite for solar cells.â€

- LLM: â€œNeed band gap + stability data.â€
â†’ Tools used
â†’ LLM checks if enough data exists
â†’ Might say: â€œNeed additional papers from arXiv for comparison.â€

âœ” Safety / Reliability

- LLM decides if:

- - Results are incomplete

- - Conflicting data exists

- - More querying needed

Or final answer is safe + ready for user

âœ” Fully autonomous research chain

- This is closer to â€œmini-research agentâ€ rather than a search engine.

## ðŸ§± Recommended Architecture With FastAPI + Streamlit
```Bash
Streamlit UI
    â†“
FastAPI Backend
    â†“
LangGraph Agent (Recursive)
    â†“
LLM â†” Tools â†” LLM
    â†“
Final Answer
```


## ðŸŒŸ Why the Second LLM Pass Is the Key to Future Extensions

When the agent finishes retrieving data (from mp_api, arxiv_api, etc.), and that raw data flows back into the LLM, you unlock the ability to:

### ðŸ”® 1. Apply Custom Logic in Future Versions
Example extensions:

Screening materials using domain-specific rules

Running ML models (band gap prediction, crystal stability models)

Multi-criteria optimization (Pareto-front)

Suggesting synthesis methods based on research trends

Running automated literature meta-analysis

Re-evaluating correctness or relevance

Asking follow-up questions automatically

Because the LLM sees the full intermediate data, it can reason about:

What is missing

What is inconsistent

What needs deeper analysis

Whether another tool is needed

### ðŸ”¬ 2. Build a Fully Autonomous Research Agent (Future Goal)

This architecture is ideal if you want to eventually build a:

â€œMaterials Auto-Research Agentâ€

that can:

Fetch data

Check quality

Analyze patterns

Decide next steps

Iterate without human prompts

The second-pass LLM allows multi-step reasoning like:

â€œBand gap is available, but stability data is missing â†’ call tool B.â€

â€œLiterature says A, database says B â†’ need cross-validation.â€

â€œTwo papers contradict â€” summarize differences.â€

This is only possible because the LLM is given the raw tool outputs for deeper reasoning.

### âš™ï¸ 3. Add Future Data Processing Modules Before the Second Pass

You can insert any future module between the tool outputs and the second LLM pass, for example:

Possible future modules:

Material-property calculators

Machine-learning prediction models

Phase diagram solvers

Crystallographic analysis

DFT data post-processing

Data validation pipeline

All these modules can be added WITHOUT changing the frontend or the decision logic â€” only plug into the pipeline before LLM pass 2.

#### ðŸ” 4. Multi-Step Loops Become Natural

With this design, the LLM can do iterative reasoning:
```Bash
LLM â†’ Tools â†’ Data â†’ LLM â†’ Tools â†’ Data â†’ LLM â†’ â€¦ â†’ Final Answer
```

This future-proofs your system for:

Multi-hop scientific reasoning

Iterative querying

Long research workflows

LangGraph supports loop nodes, which make this trivial.

### ðŸ§  5. Advanced Behaviors Become Possible Later:
âœ” Data consistency checking
âœ” Knowledge-graph creation
âœ” Workflow planning
âœ” Materials screening pipelines
âœ” Experiment planning
âœ” Automated hypothesis generation

The key is:
LLM needs full access to tool results, history, and context to make smart decisions.

Your architecture supports this perfectly.

## ðŸŽ¯ Why Streamlit Is the Better Choice at This Stage

Since your system involves:

FastAPI backend

LangGraph / Agent reasoning loops

Second-pass LLM logic

Research data tables

Material properties visualization

Graphs and charts

Ability to extend with future scientific modules

Streamlit gives you all of this easily, without touching JavaScript.

âœ” Best for rapid development

You will iterate fast.

âœ” Python-only

No React, no HTML, no JS â€” perfect for scientific workflows.

âœ” Built-in charts, tables, dataframe viewer

Great for material properties, band gap plots, etc.

âœ” Easy integration with FastAPI

Streamlit â†’ HTTP â†’ FastAPI â†’ Agent â†’ Tools â†’ LLM
Smooth and simple.

âœ” Perfect for scientific dashboards

You can show:

tables of mp_api results

plots (matplotlib, plotly)

PDF/abstract previews

structure information

âœ” Ideal while the system is evolving

As you build:

decision loops

new tools

custom processors
Streamlit adapts easily.

## âœ… 1. User-Facing UI

Clean

Simple

Only the final results

No agent graph

No internal reasoning

Professional interface

This could be:

ðŸ‘‰ Agent ChatUI (Recommended for users)

A beautiful user-facing chat interface with streaming, tool-call visualization, etc.

OR

ðŸ‘‰ Streamlit Light Mode

If you want a dashboard-style research interface.

## âœ… 2. Developer-Facing UI

This is where you:

Debug agents

Visualize LangGraph

Inspect control flow

Add tools or sub-agents

View tool outputs

Examine internal steps

See tokens, messages, errors

Run â€œdry-runâ€ modes

For this developer UI, the best tool is Streamlit or a custom FastAPI Admin panel.

And LangGraph natively supports graph visualization, so you can plug it into Streamlit or a local admin dashboard.

```Bash
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Developer Interface      â”‚
                    â”‚  (Streamlit + LangGraph viz)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Interface        â”‚           â”‚   FastAPI Backend          â”‚
â”‚ (Agent ChatUI / Streamlit)â”‚        â”‚ (Router + LangGraph Agent) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚         Tools               â”‚
                                   â”‚ (mp_api, arxiv, python...) â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
## ====================================================================
âœ… 1. AGENT (graph.py)
## ====================================================================

- (1) Intent Agent â†’ maps query to tool category
- (2) Tool-Decision Router â†’ selects the correct tool
- (3) Tools Layer â†’ actual API calls
- (4) Meta-Agent â†’ continue vs return

ðŸŽ‰ Our agent now supports MULTIPLE REAL TOOLS!
âœ” arXiv search
âœ” Materials Project (Chemistry + Materials Science)
âœ” Vector Search via FAISS
âœ” Python compute
âœ” LLM-based intent detection
âœ” LLM-based meta-decision
âœ” Automatic looping
âœ” Fully integrated LangGraph flow

## ============================================================
âœ… 2. Updated ui_main.py (Frontend Chat UI)
- - To Run # streamlit run ui_main.py
## ============================================================
âœ” Clean CLI UI
âœ” Shows which tool the agent selected
âœ” Shows errors
âœ” Pretty formatting

## ============================================================
âœ… 3. Updated ui_admin.py (Streamlit Admin Dashboard)
## ============================================================
âœ” Full graph debugging view
âœ” Shows tool selection
âœ” Shows raw tool output
âœ” Shows meta-reasoning
âœ” Real-time trace viewer
âœ” Beautiful layout

## ============================================================
ðŸ“¦ 4. Backend (FINAL) + SQL log

- - To Run # uvicorn backend:app --reload
## ============================================================
âœ… Key Features

- Session-based logging: Each chat belongs to a session_id.
- Timestamps: Every message logged with UTC timestamp.
- Role tracking: user vs agent.
- Tool & Raw Data: Tools used and raw outputs saved.
- History endpoint: /history/{session_id} to fetch full chat logs.
- Automatic UUID generation if session not provided.

## ============================================================
ðŸ“¦ 5. Updated Requirements (FINAL)
## ============================================================
fastapi
uvicorn
openai>=1.0.0
langgraph
langchain-community
streamlit
streamlit-json
requests
pydantic
python-dotenv

# Tools
arxiv
mp-api
faiss-cpu
numpy

# database
sqlalchemy
databases

# =============================================================
ðŸŽ‰ DONE â€” All components fully updated!
Your system now supports:
ðŸ¤– Multi-Tool Research Agent (LangGraph)

arXiv search

Materials Project

Python compute

Vector search (FAISS)

ðŸ”„ Multi-step workflow

Intent agent

Tool router

Tool execution

Data analyzer

Meta-agent

Loop until complete

Final answer agent

ðŸ–¥ Two UIs

Clean CLI chat

Full admin/debug dashboard

ðŸš€ REST API Backend

Perfect for production

Trace-enabled

CORS-enabled

ðŸ‘‰ WANT MORE?


ðŸ”¹ A React web UI for the user chat front-end
ðŸ”¹ A Docker Compose setup
ðŸ”¹ A PostgreSQL/Redis memory store
ðŸ”¹ A vector DB integration (LanceDB / Pinecone)


# To run Backend
```Bash
uvicorn backend:app --reload
```

# To run Frontend
```Bash
streamlit run ui_main.py
```