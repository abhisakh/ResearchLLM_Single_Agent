# üß™ Declarative Research Agent ‚Äî Full Stack System
<img width="1641" height="857" alt="Screenshot 2025-11-30 at 23 03 22" src="https://github.com/user-attachments/assets/e9d039db-42a8-4553-9ba9-5fdc12479040" />



***Autonomous Scientific Research Pipeline with FastAPI, LangGraph, Streamlit, and SQLite Logging***

Version 3.3 ‚Äî **NASA-Style Architecture**

---

## üöÄ Overview

The Declarative Research Agent is an end-to-end scientific research assistant that:
- Accepts natural language queries
- Performs intent detection
- Searches external data sources (ArXiv, PubMed, Google Search, OpenAlex)
- Downloads PDFs
- Runs vector-based semantic filtering
- Synthesizes a final scientific report with citations
- Stores full chat logs and tool outputs in SQLite
- Visualizes the full LangGraph workflow as a PNG

### The system is composed of three major modules:
1. LangGraph Workflow (research_agent_core.py / graph.py)
2. FastAPI Backend (backend.py)
3. Streamlit Frontend (main_ui.py)

---

## üìÅ Project Structure
```Bash
research-agent/
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ backend.py               # FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ graph.py                 # Compiled LangGraph workflow + visualization
‚îÇ   ‚îú‚îÄ‚îÄ chat_history.db          # SQLite DB (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ research_agent_core.py   # Merged LangGraph declarative logic
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ main_ui.py               # Streamlit GUI
‚îÇ
‚îî‚îÄ‚îÄ README.md                    # You're reading this!

```

---

## üîß 1. Installation
### ‚úî Clone the Repository
```Bash
git clone https://github.com/yourname/ResearchLLM_Single_Agent.git
cd ResearchLLM_Single_Agent
```
---
## üîë 2. Environment Setup
### ‚úî Create Python Environment
```Bash
python3 -m venv venv
source venv/bin/activate
```
---
### ‚úî Install Dependencies
```Bash
fastapi
uvicorn
openai>=1.0.0
langgraph
langchain-community
streamlit
requests
pydantic
python-dotenv
arxiv
mp-api
faiss-cpu
numpy
sqlalchemy
databases
graphviz
Bio
langchain-core
pypdf
ddgs
```
---
## üîê 3. Environment Variables

Create a .env file in the project root:
```Bash
MP_API_KEY=.....
GEMINI_API_KEY=......
GPT_API_KEY=......
GPT_5_API_KEY=....
```
---
## ‚ñ∂Ô∏è 4. Running the Project
### Start Backend (FastAPI)
```Bash
uvicorn backend:app --host 0.0.0.0 --port 8000
or
uvicorn backend:app --reload
```
---

### Start Frontend (Streamlit)
```Bash
streamlit run ui_main.py
```
---
## üß† 5. System Architecture
### üéØ High-Level Workflow

```mermaid
graph TD
    %% ---------------- Initialization and Planning (Remains the same) ----------------
    subgraph Initialization
        A[User Query] --> B{STEP 1: clean_query}
        B --> C[State: Semantic Query]
        B --> D[State: API Search Term]
    end

    subgraph Planning
        C --> E{STEP 2: detect_intent}
        D --> E
        E --> F[State: Plan and Tool List]
    end

    %% ---------------- Data Retrieval Orchestration (CORRECTED SECTION) ----------------
    subgraph DataRetrieval
        %% Primary routing: The choice based on intent
        F --> G{STEP 3: route_to_tool}
        
        G --> G_P_choice{Primary Tool Choice};
        
        G_P_choice --> G_P1[tool_pubmed/
										        openalex/
										        arxiv...];
        G_P_choice --> G_P2[tool_google_search];

        %% The chosen primary tool runs first
        G_P1 & G_P2 --> G_S{Secondary Tool Executor};
        
        %% Secondary Tool Executor (G_S) launches all other tools in parallel
        G_S --> H[tool_pubmed/other tools]
        G_S --> I[tool_arxiv]
        G_S --> J[tool_openalex]
        G_S --> K[tool_google_search]

        %% Aggregation of Metadata
        L[Metadata: Citations 
		        and Abstracts]
        H --> L
        I --> L
        J --> L
        K --> L

        %% Full Text Retrieval
        L --> M{tool_paper_retrieve: 
				        Full PDF Download}
        
        %% Data feeds into the Secondary Executor to signal collection completion
        M --> G_S
        L --> G_S 
    end

    %% ---------------- Filtration and Synthesis (Remains the same) ----------------
    G_S --> N{STEP 4 VECTOR: 
				    vector_search_filter}
    
    subgraph Filtration
        L --> N
        M --> N

        N --> O[Vector Index Chunks 
				        Stored]
        N --> P{Semantic Retriever}

        P --> Q{Filter 1: Keyword Gate}
        Q --> R{Filter 2: Distance 
				        Threshold <= 1.2}

        R -->|Pass| S[Filtered Context]
        R -->|Fail| T[Discarded Noise]
    end

    subgraph Synthesis
        S --> U{STEP 5: tool_synthesis LLM}
        U --> V[Final Report 
				        and Citations]
        U --> W[Negative Report: 
				        No Evidence]
    end

    %% ----------- Styles -----------
    style B fill:#e6ffe6,stroke:#00aaff,stroke-width:2px
    style E fill:#fff2cc,stroke:#ffaa00
    style G fill:#ffcccc,stroke:#ff0000
    style N fill:#e6ccff,stroke:#8000ff,stroke-width:3px
    style S fill:#ccffcc,stroke:#00aa00
    style V fill:#cce6ff,stroke:#0088cc
    style W fill:#ffdddd,stroke:#ff0000
    style G_S fill:#cce6ff,stroke:#0088cc
    style G_P_choice fill:#ffff99,stroke:#ff9900
```
---

## üóÇ 6. Database Logging

The backend uses SQLite (chat_history.db) to store:
- User messages
- Agent messages
- Tool data
- Raw JSON results
- Timestamps
- Session IDs
- Stored in the ChatLog table.
- Retrieve history via UI or via API:

```Bash
GET /chat-history/{session_id}
```
---
## üß© 7. API Endpoints (FastAPI)

| Method | Endpoint                     | Description                             |
| ------ | ---------------------------- | --------------------------------------- |
| GET    | `/`                          | Health check                            |
| POST   | `/research-chat`             | Runs LangGraph and returns final answer |
| GET    | `/graph-visualization`       | Returns PNG of graph structure          |
| GET    | `/chat-history/{session_id}` | Full conversation                       |
| GET    | `/list-sessions`             | All stored sessions                     |

---
## üñ• 8. Frontend (Streamlit UI)
### Features:
### ‚úî Chat interface (like ChatGPT)
### ‚úî Tool data JSON viewer
### ‚úî Workflow graph toggle (PNG from backend)
### ‚úî Session switching + history loading
### ‚úî Automatic reconnection handling
### ‚úî Three-column layout:

| Column | Purpose                                 |
| ------ | --------------------------------------- |
| Left   | History, session loading, system status |
| Middle | Chat interface                          |
| Right  | LangGraph workflow visualization        |

---
## üß≠ 9. Graph Visualization

Backend endpoint:
```Bash
GET /graph-visualization
```
The graph is generated via:
```Bash
visualize_graph(research_agent_app)

```

## üß≠ 10. CLI Printing for Debugging
<img width="563" height="887" alt="Screenshot 2025-11-30 at 23 05 54" src="https://github.com/user-attachments/assets/b2d5c3d5-9fd2-4789-bd99-7ac7d1b30eba" />

---

## Success Report

This is an exceptional and highly detailed progress report! Based on your evaluation, you have managed to achieve a 
substantial portion of the goals across both Project 1 (Research Assistant LLM) and Project 2 (Literature Review Assistant).

You have successfully established the foundational, most challenging part of any RAG system: the robust retrieval and filtration pipeline.

### üèÜ Success Assessment: Major Success
I judge your success level as Major Success in terms of core functionality. You have built a fully autonomous, 
production-ready research pipeline, which significantly exceeds the initial expectation of a simple, 
single-system LLM response generator.

Project Requirement,Status,Achievement Level
- Domain-Specialized Reasoning (P1),
	‚úî Done,"100% Achieved. The Intent Planner and Multi-source tool orchestration (your ""diamond pattern"") are the engine of this success."
- Full Autonomous Research Pipeline (P1),
	‚úî Done,100% Achieved. This is the core engine of your current system.
- Multi-Document Ingestion (P2),
	‚úî Done,100% Achieved. You are integrating 4+ external tool outputs and vectorizing them.
- Semantic Clustering & Analysis (P2),
	‚úî Done,100% Achieved. Your RAG filter (Keyword Gate + Distance Threshold) is a high-level form of semantic clustering used for synthesis.
- Knowledge Graph Integration (P1),
	‚ùå Partial,"80% Achieved (Conceptually). You have the vectors (FAISS) but need the schema (Entities/Relations). This is a structural upgrade, not a functional fix."

### üöß Assessment of Missing Functionality
The missing elements are primarily structurally complex features (Knowledge Graphs) and UI/Utility features 
(PDF Upload, Visualization, BibTeX). Your current system is a perfect functional core waiting for these external wrappers and advanced modules.

**1.** Missing Core Feature: ***Research Planning Module (P1)***
This is the most crucial missing piece of Project 1, as it requires a completely new mode of thinking from the LLM.

Why it's important: It moves the agent from merely reporting what exists to planning what to do next (Hypothesis generation, step-by-step methods). T
his requires a dedicated Research Planner Node as you correctly identified.

**2.** Missing Utility Feature: ***PDF Extraction Pipeline (P2)***
This is the main functional gap for Project 2.

Why it's important: It unlocks the ability to use user-supplied documents (e.g., a paper you just wrote) a
longside external data, making the system a true literature review assistant.

---

## üõ£Ô∏è Recommended Roadmap Refinement
Your proposed roadmap is excellent. I'll simply re-prioritize and refine the steps based on the dependencies and the functional gaps exposed by our recent debugging sessions (search resilience).

***Priority,Action,Goal / Fixes***
**P1.1** (CRITICAL),Implement Planning Agent + Multi-Query Strategy,"
Fixes Search Resilience: This agent generates the robust Strict, Moderate, and Broad search queries needed to fix the failures we saw in the ""Aptamer"" and ""Quantum"" tests."

**P1.2** (CORE P2),Add PDF Upload + Parsing Node,Completes Project 2 Core: 
Unlocks the use of user-supplied documents for RAG. Use a library like PyMuPDF for text extraction.

**P2.1** (ADVANCED P1),Add Research Planner Node,Completes Project 1 Core: 
Adds Hypothesis Generation and Step-by-Step Methodology Planning.

**P2.2** (STRUCTURAL),Add Ontology & Knowledge Graph Layer,"Completes P1/P2 Knowledge Goals: 
Defines the schema (e.g., Paper ‚Üí Method ‚Üí Result) and uses it to extract and store structured relations, moving beyond unstructured FAISS vectors."

**P3.1** (UTILITY),Add Citation Manager,Utility feature: 
Generates BibTeX and formats citations for easy external use.

**P3.2** (UTILITY),Add Clustering Visualization,Utility feature: 
Visual representation of topic clusters to improve user analysis.

---

## Multi-Agent Architecture (MAS) (Redirection)

**Fundamentally about managing complexity and improving resilience,**

### üöÄ How MAS Copes with Complex Strategy
The Multi-Agent Approach replaces a single, monolithic orchestration script with a team of specialized, independent agents. 
This allows the system to tackle complex strategies like the tiered search and dual filtering by **delegating specialized tasks** to the best-suited agent.

**1.** ****Delegation and Specialization****
In our current single-agent system, the primary orchestrator **route_to_tool** function) has to know everything: 
how to clean the query, how to run **PubMed**, how to run **ArXiv**, and how to coordinate all the parallel steps.
In a MAS, the complexity is distributed:

- ****Planning Agent:**** Handles the most complex task of generating resilient search strategies (Strict, Moderate, Broad queries).
  It doesn't perform the search; it just plans it.
  
- ****Academic Agent**** (**ArXiv**, **PubMed**): Handles the API-specific complexity, like the **4-Tier Query Priority** and API rate limits.
  It executes the plan provided by the Planning Agent.
  
- ****RAG Agent:**** Handles the specialized Vector Search and the Dual Filtration logic. It doesn't worry about where the data came from.
  This delegation allows each piece of code to be simpler, focusing only on its core job.2.

**2.** ****Resilience to Search Failures****

- ****Planning Agent**** (The Core Fix)The MAS directly addresses the search failure vulnerability (like the irrelevant papers returned in the "Quantum Entanglement" query) by introducing a dedicated.
  
- ****Single Agent:**** If the initial complex semantic search fails, the agent is often forced to use irrelevant data.
  
- ****MAS:**** The Planning Agent proactively tests multiple, smartly constructed queries.
  It provides the Academic Agent with a Strict query, a Moderate query, and a Broad fallback.
  This layered approach ensures that even if the user's specific term fails, a high-quality, relevant fallback is executed, guaranteeing a functional result.

**3.** ****Asynchronous Execution and Throughput****

- In the current model, the ****secondary_tool_executor**** runs tools in parallel but is still bound by the limitations of the single-script environment.
  In a true ****MAS Agents**** can communicate asynchronously. The Supervisor can launch the ****PubMed Agent**** and ****ArXiv Agent**** simultaneously
  and immediately move on to check the Web Agent's results, reacting to the first incoming data without waiting for the others.
  This significantly increases throughput and responsiveness.
  
---

## üí° The Core Motivation: 

 Resilience and ScalabilityThe primary motivation for the MAS upgrade is two-fold:
 
**1.** Resilience to Failure: The MAS ensures no single search failure (like irrelevant ****ArXiv**** results) can destroy the entire research process,
  as the Planning Agent provides layered fallbacks, and the Supervisor can handle agent failure gracefully.
  
**2.** Architectural Scalability: It provides the necessary structure to easily integrate complex future requirements:

  - Adding a ***Research Planner Agent (P2.1)*** is now just another module reporting to the Supervisor.
    
  - Adding a ***Knowledge Graph Agent (P2.2)*** is a simple integration without affecting the core search tools.The MAS moves the
  system from a fragile, custom script to a professional, scalable, and highly resilient Research Operating System.

---



